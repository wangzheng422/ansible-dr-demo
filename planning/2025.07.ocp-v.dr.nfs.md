> [!NOTE]
> work in progress
# OpenShift Virtualization Disaster Recovery based on storage replication solution.

In Red Hat's solution, OpenShift Virtualization (OCP-V) does not have a built-in DR solution; Red Hat recommends using ODF's DR feature for this purpose. Essentially, this is a storage-based solution. ODF supports Ceph or IBM Flash Storage as backend storage solutions. However, if customers use third-party storage solutions like Dell, EMC, or Hitachi, how should an OCP-V DR solution be implemented?

Our approach leverages OpenShift's OADP (OpenShift API for Data Protection) feature to back up OCP-V metadata, including virtual machine configurations and Persistent Volume (PV)/Persistent Volume Claim (PVC) configurations. Concurrently, we utilize the underlying storage solution's remote replication capabilities to a remote site. Should the primary site encounter an issue, the DR process is manually triggered: OCP-V virtual machines at the primary site are shut down, and storage is set to read-only. Subsequently, the underlying storage's remote replication is verified or initiated to synchronize PVs/LUNs, and the completion of the backup process is confirmed. Finally, the OCP-V metadata is imported at the remote site, and storage system-related information within the PVs is updated to ensure PVs correctly map to the appropriate storage LUNs or directories. This completes the DR process.

The process for switching back from the remote site to the primary site is identical.

This document uses NFS to simulate the underlying storage system to demonstrate an OCP-V DR process. It begins with manual operations, followed by automating the entire process with the assistance of Generative AI and Ansible.

![](files/ocp-v.dr.01.drawio.png)

# install ocp-v

Our demo case is OCP-V, focusing on VM DR, so the `OpenShift Virtualization` operator needs to be installed.

<img src="imgs/2025.07.ocp-v.dr.nfs.md/2025-07-24-11-34-02.png" width="1024">

<img src="imgs/2025.07.ocp-v.dr.nfs.md/2025-07-24-11-34-18.png" width="1024">

<img src="imgs/2025.07.ocp-v.dr.nfs.md/2025-07-24-11-36-30.png" width="1024">

Because we have customized NFS storage, we need to ensure CNV recognizes it.

```bash

cat << EOF > $BASE_DIR/data/install/cnv-sp.yaml
apiVersion: cdi.kubevirt.io/v1beta1
kind: StorageProfile
metadata:
  name: nfs-dynamic
spec:
  claimPropertySets:
  - accessModes:
    - ReadWriteMany
    volumeMode: Filesystem
EOF

oc apply -f $BASE_DIR/data/install/cnv-sp.yaml

```

# install oadp

We use OpenShift's official OADP (OpenShift API for Data Protection) operator for remote metadata backups. OADP is an S3-based remote backup solution that can back up both metadata and PVC data. However, since it compresses data and uploads it to S3, then downloads and decompresses it during recovery, this process is too slow for virtual machines.

Therefore, our solution naturally involves using OADP to back up metadata, while leveraging the storage system's native capabilities for remote data replication.

First, we install OADP on both the local (cluster-01) and remote (cluster-02) clusters.
<img src="imgs/2025.07.ocp-v.dr.nfs.md/2025-07-24-15-56-54.png" width="1024">

Next, we configure the S3 access credentials.

```bash
cat << EOF > $BASE_DIR/data/install/credentials-minio
[default]
aws_access_key_id = minioadmin
aws_secret_access_key = minioredhat
EOF

oc create secret generic minio-credentials \
--from-file=cloud=$BASE_DIR/data/install/credentials-minio \
-n openshift-adp
```

Finally, we create an OADP instance, specifying the S3 storage location to be used.

```bash
# define the oadp instance
# oc delete -f $BASE_DIR/data/install/oadp.yaml
cat << EOF > $BASE_DIR/data/install/oadp.yaml
apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: velero-instance
  namespace: openshift-adp # The namespace where the OADP Operator is located
spec:
  # 1. Define your S3 bucket information here
  backupLocations:
    - name: default
      velero:
        provider: aws # or gcp, azure, etc.
        default: true
        objectStorage:
          bucket: ocp
          prefix: velero # Backups will be stored in the velero/ directory inside the bucket
        config:
          # For non-AWS S3, for example MinIO, you need to provide the URL
          s3Url: http://192.168.99.1:9000
          region: us-east-1
        # Reference the Secret containing your S3 credentials
        credential:
          name: minio-credentials
          key: cloud
  
  # 2. Configure Velero's core behavior and plugins here
  configuration:
    velero:
      # Specify the plugins to use
      defaultPlugins:
        - openshift
        - kubevirt
        - csi
        - aws
        
    # 3. Configure volume snapshots and data movers (DataMover/Kopia) here
    csi:
      enable: false
    datamover:
      enable: false
EOF

oc apply -f $BASE_DIR/data/install/oadp.yaml
```

# Main site(cluster-01) create backup

On the primary site, we have a `demo` project where a virtual machine has already been created.

<img src="imgs/2025.07.ocp-v.dr.nfs.md/2025-07-29-15-17-24.png" width="1024">

Let's get the information of the vm, and the pvc and pv associated with the vm.

```bash
oc get vm -n demo
# NAME                                AGE    STATUS    READY
# centos-stream10-aqua-spoonbill-10   3d5h   Running   True

oc get vm centos-stream10-aqua-spoonbill-10 -n demo -o jsonpath='{.spec.template.spec.volumes[*].dataVolume.name}' && echo
# centos-stream10-aqua-spoonbill-10-volume


oc get pvc centos-stream10-aqua-spoonbill-10-volume -n demo -o jsonpath='{.spec.volumeName}' && echo
# pvc-7147333f-2db5-4b3f-9320-aac8da5170e2
```

On the primary site (cluster-01), we create an OADP backup specifically for the namespace, explicitly excluding Persistent Volume (PV) data.

```bash
# current testing test
# oc delete -f $BASE_DIR/data/install/oadp-backup.yaml
cat << EOF > $BASE_DIR/data/install/oadp-backup.yaml
apiVersion: velero.io/v1
kind: Backup
metadata:
  name: vm-full-metadata-backup-03
  namespace: openshift-adp
spec:
  # 1. Include the namespace containing your VMs and PVCs
  includedNamespaces:
    - demo

  # 2. Explicitly include PVs in the backup
  # By default, Velero automatically includes PVs bound by PVCs in the backup.
  # However, to be absolutely certain and explicit, you can specify them.
  # Note: Velero's intelligence usually makes this unnecessary if you are backing up the PVCs.
  # includedResources:
  #   - persistentvolumes
  #   - persistentvolumeclaims
    # - virtualmachines  (and any other resources you need)
    # If you leave this field empty, it backs up all resources in the includedNamespaces.

  # 3. Include cluster-scoped resources like PVs
  # When backing up a namespaced resource (PVC) that depends on a cluster-scoped
  # resource (PV), Velero typically includes the PV automatically. Setting this to 'true'
  # ensures all cluster-scoped resources are considered. For just capturing PVs
  # linked to your PVCs, the default 'nil' (auto) is usually sufficient.
  includeClusterResources: true

  # 4. The MOST IMPORTANT setting for your use case:
  # This tells Velero "Do NOT attempt to create a data snapshot of the PVs."
  # It will still save the PV object's YAML definition.
  snapshotVolumes: false

  # Define where to store the metadata backup
  storageLocation: default

  # Set a Time To Live for the backup object
  # ttl: 720h0m0s # 30 days
  ttl: 3h0m0s
EOF

oc apply -f $BASE_DIR/data/install/oadp-backup.yaml

```


# Recovery (DR site, cluster-02)

At the DR site, the first step is to synchronize the data using the storage system's capabilities. In our NFS scenario, we can simply use the `rsync` tool to copy the directory.

```bash

sudo rsync -avh --progress /srv/nfs/openshift-01/demo-centos-stream10-aqua-spoonbill-10-volume-pvc-7147333f-2db5-4b3f-9320-aac8da5170e2 /srv/nfs/openshift-02/
```

Next, we manually locate the `pvc` (Persistent Volume Claim) and its metadata that needs to be synchronized from cluster-01. We then modify it according to the configuration of cluster-02 and create it on cluster-02.

In our NFS scenario, the `path` within the PV needs to be modified. For other storage backends, the relevant parameters should be replaced according to the specific situation.

```yaml
kind: PersistentVolume
apiVersion: v1
metadata:
  name: pvc-7147333f-2db5-4b3f-9320-aac8da5170e2
spec:
  capacity:
    storage: '36071014400'
  nfs:
    server: 192.168.99.1
    path: /srv/nfs/openshift-02/demo-centos-stream10-aqua-spoonbill-10-volume-pvc-7147333f-2db5-4b3f-9320-aac8da5170e2
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Delete
  storageClassName: nfs-dynamic
  volumeMode: Filesystem
```

Next, we create the PVC. Generally, OADP should not directly restore PVCs, as this would trigger automatic PV deployment on the new cluster.

```yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: centos-stream10-aqua-spoonbill-10-volume
spec:
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: '36071014400'
  volumeName: pvc-7147333f-2db5-4b3f-9320-aac8da5170e2
  storageClassName: nfs-dynamic
  volumeMode: Filesystem
```

Next, we create the `restore` object. This will automatically recreate the virtual machines and their associated configurations from the original `demo` project on the target cluster (cluster-02) within the same project.

```bash
# oc delete -f $BASE_DIR/data/install/oadp-restore.yaml
cat << EOF > $BASE_DIR/data/install/oadp-restore.yaml
apiVersion: velero.io/v1
kind: Restore
metadata:
  name: restore-metadata-excluding-pvs-03
  namespace: openshift-adp
spec:
  # 1. Specify the name of the backup you want to restore from.
  backupName: vm-full-metadata-backup-03

  # 2. Exclude PersistentVolumes and PersistentVolumeClaims.
  # This is the most critical part for your request.
  excludedResources:
    - persistentvolumes
    - persistentvolumeclaims

  # 3. (Optional) Map the original namespace to a new one if needed.
  # If you want to restore to the same namespace ('demo'), you can omit this.
  # If you want to restore to a different namespace (e.g., 'restored-demo'), use this.
  # namespaceMapping:
  #  demo: restored-demo

  # Let Velero restore cluster-scoped resources (like CRDs, etc.) that were included in the backup.
  # The default 'auto' is usually sufficient.
  includeClusterResources: false
EOF

oc apply -f $BASE_DIR/data/install/oadp-restore.yaml

```

Now we can see the vm is created in cluster-02, it is in `paused` status because the demo system has limited io resource. 

<img src="imgs/2025.07.ocp-v.dr.nfs.md/2025-07-29-15-18-30.png" width="1024">

# if something wrong

If the recovery process encounters issues and the VM fails to start, you can delete the VM and attempt to recreate the `restore` object.

```bash
# Delete the stuck VM
oc delete vm centos-stream10-aqua-spoonbill-10 -n demo
```

# Regular backups

In a real-world scenario, we would certainly implement scheduled backups, backing up the target project's metadata hourly. OADP provides this functionality.

```bash
# oc delete -f $BASE_DIR/data/install/oadp-schedule.yaml
cat << EOF > $BASE_DIR/data/install/oadp-schedule.yaml
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: daily-metadata-backup-schedule
  namespace: openshift-adp
spec:
  # 1. Define the backup schedule (using Cron expressions)
  # This example is for "22 min hourly"
  schedule: 22 * * * *

  # 2. Defining a backup content template. (template)
  # This section is almost identical to the Backup object you created earlier.
  template:
    spec:
      # Include the namespaces you need to back up.
      includedNamespaces:
        - demo

      # Ensure cluster-scoped resources are included.
      includeClusterResources: true

      # Most important setting: Do not take volume snapshots; only back up metadata.
      snapshotVolumes: false

      # Backup storage location
      storageLocation: default

      # 3. Set the backup retention policy. (TTL - Time To Live)
      # This backup will be automatically deleted 30 days after creation.
      # Format is hours (h)/minutes (m)/seconds (s).
      # ttl: 720h0m0s # (30 days * 24 hours = 720 hours)
      ttl: 3h0m0s
EOF

oc apply -f $BASE_DIR/data/install/oadp-schedule.yaml

```

OADP will automatically create backups with names like `daily-metadata-backup-schedule-20250730010000`, and these will be automatically deleted upon expiration.

# Automating the DR Process

The manual steps outlined above can be scripted to create a fully automated disaster recovery workflow. Here is a high-level overview of the automation logic:

1.  **Schedule Backups**: On the primary cluster, create a recurring OADP `Schedule` to regularly back up application metadata.

2.  **Extract and Analyze**: An automation script will periodically download the latest backup from S3. It will then decompress the archive and parse the manifest files to identify the specific `PersistentVolume` (PV) and `PersistentVolumeClaim` (PVC) resources associated with the protected virtual machines.

3.  **Synchronize and Prepare**:
    *   Using the details from the PV manifests, the script will trigger the underlying storage system's remote replication to the DR site.
    *   The script will then transform the PV manifest, updating storage-specific parameters (e.g., NFS server IP, LUN ID) to match the configuration of the DR site.
    *   Finally, it will apply the modified PV and PVC manifests to the DR cluster to pre-stage the volume resources.

4.  **Execute Failover**: In the event of a disaster, the final automated step is to run an OADP `Restore` on the DR cluster. This restore will bring the VM and other application resources online, connecting them to the already replicated and pre-staged persistent volumes.

# end